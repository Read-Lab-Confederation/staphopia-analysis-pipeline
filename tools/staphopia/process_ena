#! /usr/bin/env python
""" Process ENA experiments. """

import os
import sys
import socket

from staphopia.config import BIN
from staphopia.tasks import shared


def create_job_script(fastq_files, working_dir, sample_tag):
    job_script = '{0}/{1}.sh'.format(working_dir, sample_tag)
    input2 = '--fastq2' if len(fastq_files) == 2 else ''
    fastq2 = fastq_files[1] if len(fastq_files) == 2 else ''
    stdout, stderr = shared.run_command(
        ['python', BIN['create_job_script'],
         fastq_files[0],
         input2, fastq2,
         '--working_dir', working_dir,
         '--processors', '8',
         '--sample_tag', sample,
         '--ena', '--log_times'],
        stdout=job_script
    )

    return job_script


def update_status(experiment, status, server, path):
    stdout, stderr = shared.run_command(
        ['python', BIN['manage'], 'update_status',
         experiment,
         status,
         '--server', server,
         '--path', path],
        verbose=False
    )


def get_samples(limit, technology, coverage, min_read_length):
    import json

    stdout, stderr = shared.run_command(
        ['python', BIN['manage'], 'unprocessed_ena',
         '--limit', str(limit),
         '--technology', technology,
         '--coverage', str(coverage),
         '--min_read_length', str(min_read_length)],
        verbose=False
    )
    return json.loads(stdout).keys()


if __name__ == '__main__':
    import argparse as ap
    import socket
    import glob
    import time
    server = socket.gethostname()
    parser = ap.ArgumentParser(
        prog='process_ena',
        conflict_handler='resolve',
        description=('')
    )
    parser.add_argument('download_dir', metavar="DOWNLOAD_DIRECTORY", type=str,
                        help='Directory to download ena data to.')
    parser.add_argument('working_dir', metavar="WORKING_DIRECTORY", type=str,
                        help='Directory to process sampels in.')
    parser.add_argument('--limit', metavar="INT", type=int, default=4,
                        help='Number of samples to queue up.')
    parser.add_argument('--technology', metavar="STR", type=str,
                        default='ILLUMINA',
                        help='Sequencing technolgy.')
    parser.add_argument('--coverage', metavar="INT", type=int,
                        default=100, help='Minimum coverage.')
    parser.add_argument('--min_read_length', metavar="INT", type=int,
                        default=90, help='Minimum read length.')

    args = parser.parse_args()

    # Get samples to Process
    samples = get_samples(args.limit, args.technology, args.coverage,
                          args.min_read_length)

    for sample in samples:
        path = "{0}/{1}".format(args.working_dir, sample)
        update_status(sample, 'init', server, path)

    for sample in samples:
        path = "{0}/{1}".format(args.working_dir, sample)
        # Update status
        update_status(sample, 'downloading', server, path)

        # Download
        print("Downloading {0} FASTQ".format(sample))
        stdout, stderr = shared.run_command(
            ['python', BIN['download_ena'],
             args.download_dir,
             BIN['manage'],
             '--experiment', sample]
        )
        print(stdout)

        # Update Status
        update_status(sample, 'downloaded', server, path)

        fastq_files = []
        directory = '{0}/{1}*.fastq.gz'.format(args.download_dir, sample)
        for fastq in sorted(glob.glob(directory)):
            suffix = ''
            if '_R1' in fastq:
                suffix = '_R1'
            elif '_R2' in fastq:
                suffix = '_R2'

            new_fastq = '{0}/{1}{2}.fastq.gz'.format(path, sample, suffix)
            fastq_files.append(new_fastq)

            # Make the new directory
            try:
                os.makedirs(path)
            except OSError:
                if not os.path.isdir(path):
                    raise

            # Copy or symlink the renamed fastq
            os.symlink(fastq, new_fastq)

        # Create Job Script
        job_script = create_job_script(fastq_files, path, sample)

        # Qsub Job
        stdout, stderr = shared.run_command(['qsub', job_script])
        update_status(sample, 'queued', server, path)
        print(stdout)
        time.sleep(601)
