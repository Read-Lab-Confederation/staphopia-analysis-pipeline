#! /usr/bin/env python
"""Call SNPs and InDels from the input FASTQ."""
import json
import sys

from ruffus import *

from staphopia.helpers.time_job import time_job
from staphopia.tasks import variants, shared

parser = cmdline.get_argparse(description='Call variants from input FASTQ.')
parser.add_argument("fastq", metavar="INPUT_FASTQ", help="Input FASTQ file")
parser.add_argument("reference", metavar="REFERENCE_FASTA",
                    help="Input reference FASTA file.")
parser.add_argument("genbank", metavar="REFERENCE_GENBANK", default='',
                    help="Input reference GenBank file.")
parser.add_argument("-o", "--output", dest="output", type=str,
                    help="Output directory. (Default: ./)", metavar="STR",
                    default='./')
parser.add_argument("-r", "--read_length", dest="read_length", type=int,
                    help="Mean read length of input FASTQ file", metavar="INT",
                    default=0)
parser.add_argument('-p', '--processors', metavar="INT", type=int, default=1,
                    help='Number of processors to use. (Default 1)')
parser.add_argument('--paired', action='store_true', default=False,
                    help='Input is interleaved paired end reads.', )
parser.add_argument('--tag', help='Prefix for final VCF. (Default variants)',
                    dest='tag', default='variants')
parser.add_argument('--log_times', action='store_true', default=False,
                    help='Write task run times to file (Default: STDERR).', )
options = parser.parse_args()

NUM_CPU = str(options.processors)
GATK_DIR = 'analyses/variants/gatk'
REFERENCE = '{0}/ref.fasta'.format(GATK_DIR)
TIME_LOG = sys.stderr
if options.log_times:
    TIME_LOG = 'logs/time/call_variants.txt'
# Pipeline --------------------------------------------------------------------
read_length = options.read_length
if not options.read_length:
    # Try to read a stats file
    try:
        stats = options.fastq.replace('gz', 'stats')
        with open(stats, 'r') as f:
            json_data = json.loads(f.readline().rstrip())
            read_length = json_data['mean_read_length']
    except:
        # Unknown read length, set to 0 and use BWA aln/samse
        read_length = 0


@mkdir('logs/time')
@transform(options.fastq,
           regex(r"(.*).cleanup.fastq.gz"),
           r"analyses/variants/\1.variants.vcf.gz")
@time_job(TIME_LOG, new_stream=True)
def call_variants(ref, final_vcf):
    """Call variants."""
    # Build index will local copy of reference
    shared.run_command(['rm', '-rf', GATK_DIR])
    shared.run_command(['mkdir', '-p', GATK_DIR])
    shared.run_command(['cp', options.reference, REFERENCE])
    variants.bwa_index(REFERENCE)
    variants.samtools_faidx(REFERENCE)
    variants.create_sequence_dictionary(REFERENCE)

    # Align reads
    sam = '{0}/bwa.sam'.format(GATK_DIR)
    if (read_length > 70):
        # Run BWA mem
        variants.bwa_mem(options.fastq, sam, NUM_CPU, REFERENCE,
                         options.paired)
    else:
        # Run BWA aln/samse
        sai = sam.replace('sam', 'sai')
        variants.bwa_aln(options.fastq, sai, sam, NUM_CPU, REFERENCE)

    # Sam to Bam
    sorted_bam = '{0}/sorted.bam'.format(GATK_DIR)
    variants.add_or_replace_read_groups(sam, sorted_bam)

    # Alignment filtering/improvement
    deduped_bam = '{0}/deduped.bam'.format(GATK_DIR)
    intervals = '{0}/deduped.intervals'.format(GATK_DIR)
    realigned_bam = '{0}/realigned.bam'.format(GATK_DIR)
    variants.mark_duplicates(sorted_bam, deduped_bam)
    variants.build_bam_index(deduped_bam)
    variants.realigner_target_creator(deduped_bam, intervals, REFERENCE)
    variants.indel_realigner(intervals, deduped_bam, realigned_bam, REFERENCE)
    variants.build_bam_index(realigned_bam)

    # Call variants
    raw_vcf = '{0}/raw.vcf'.format(GATK_DIR)
    variants.haplotype_caller(realigned_bam, raw_vcf, NUM_CPU, REFERENCE)

    # Filter and annotate variants
    filtered_vcf = '{0}/filtered.vcf'.format(GATK_DIR)
    annotated_vcf = '{0}/annotated.vcf'.format(GATK_DIR)
    variants.variant_filtration(raw_vcf, filtered_vcf, REFERENCE)
    variants.vcf_annotator(filtered_vcf, annotated_vcf, options.genbank)

    # Move final vcf and cleanup
    shared.run_command(['gzip', '-c', annotated_vcf], stdout=final_vcf)
    shared.run_command(['rm', '-rf', GATK_DIR])

# -----------------------------------------------------------------------------
pipeline_run(exceptions_terminate_immediately=True, verbose=5)
